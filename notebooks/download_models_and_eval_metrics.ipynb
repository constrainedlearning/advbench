{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chiche/miniconda3/envs/daug/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from advbench.datasets import MNIST, STL10\n",
    "from advbench.datasets import to_loaders\n",
    "from advbench.algorithms import ERM, Augmentation, Adversarial_Worst_Of_K, Adversarial_PGD\n",
    "from advbench.attacks import Fo_Adam\n",
    "from advbench import hparams_registry\n",
    "from advbench.lib import  misc\n",
    "import torch\n",
    "from advbench.lib.transformations import se_matrix, angle_to_rotation_matrix, se_transform\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "import tarfile\n",
    "from torch import nn\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "import os\n",
    "import sys\n",
    "import seaborn as sns\n",
    "sys.path.append('../')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from advbench.datasets import MNIST, STL10\n",
    "from advbench.datasets import to_loaders\n",
    "from advbench.algorithms import ERM, Augmentation, Adversarial_Worst_Of_K, Adversarial_PGD\n",
    "from advbench.attacks import Fo_Adam\n",
    "from advbench import hparams_registry\n",
    "from advbench.lib import  misc\n",
    "import torch\n",
    "from advbench.lib.transformations import se_matrix, angle_to_rotation_matrix, se_transform\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from humanfriendly import format_timespan\n",
    "\n",
    "\n",
    "from advbench import datasets\n",
    "from advbench import algorithms\n",
    "from advbench import attacks\n",
    "from advbench import hparams_registry\n",
    "from advbench.lib import misc, meters, plotting, logging\n",
    "from torch.cuda.amp import autocast\n",
    "from torchsummary import summary\n",
    "DEVICE = \"cuda\"\n",
    "DOWNLOAD_WEIGHTS = False\n",
    "PULL_METRICS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(weight_path, dataset, device=DEVICE):       \n",
    "        algorithm = ERM(\n",
    "        dataset.INPUT_SHAPE, \n",
    "        dataset.NUM_CLASSES,\n",
    "        hparams,\n",
    "        device).to(device)\n",
    "        w_dict = torch.load(weight_path,map_location=DEVICE)\n",
    "        #print(weight_dict.keys())\n",
    "        #if db==\"mnist\":\n",
    "        weight_dict = {}\n",
    "        for s,v in w_dict.items():\n",
    "                if \"classifier\" in s and \"model\" not in s:\n",
    "                        weight_dict[s.replace('classifier', 'classifier.model')] = v\n",
    "                else:\n",
    "                        weight_dict[s] = v \n",
    "        algorithm.load_state_dict(weight_dict)\n",
    "        return algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for STL10\n",
      "154 runs found\n",
      "Fetching data for MNIST\n",
      "212 runs found\n",
      "Fetching data for CIFAR100\n",
      "103 runs found\n"
     ]
    }
   ],
   "source": [
    "if PULL_METRICS:\n",
    "    api = wandb.Api(timeout=50)\n",
    "    dsets = [\"STL10\", \"MNIST\", \"CIFAR100\"]\n",
    "    #results_dfs = {}\n",
    "    entity = \"hounie\"\n",
    "    all_runs_list = []\n",
    "    for dataset in dsets:\n",
    "        print(\"Fetching data for {}\".format(dataset))\n",
    "        project = f\"DAug-{dataset}\"\n",
    "        base = f\"../trained_weights/{project}\"\n",
    "        Path( base ).mkdir( parents=True, exist_ok=True )\n",
    "        runs = api.runs(f\"{entity}/{project}\")\n",
    "        print(f\"{len(runs)} runs found\")\n",
    "        for run in runs:\n",
    "            if run.state == \"finished\":\n",
    "                id = run.id\n",
    "                run = api.run(f\"{entity}/{project}/{id}\")\n",
    "                tmp_path = os.path.join(base, f\"tmp\")\n",
    "                weight_dir = os.path.join(base, run.name)\n",
    "                path = os.path.join(weight_dir, f\"{run.id}.pkl\")\n",
    "                Path( weight_dir ).mkdir( parents=True, exist_ok=True )\n",
    "                results = {**run.summary, **run.config, \"weights\": path, \"id\": run.id, \"name\":run.name}\n",
    "                if DOWNLOAD_WEIGHTS:\n",
    "                    try:\n",
    "                        f = run.file(\"train-output/delta hist_ckpt.pkl\").download(tmp_path, replace=True) \n",
    "                        os.rename(f.name, path)\n",
    "                        all_runs_list.append(results)\n",
    "                    except:\n",
    "                        try:\n",
    "                            run = api.run(f\"{entity}/{project}/{id}\")\n",
    "                            f = run.file(\"train-output/loss_ckpt.pkl\").download(tmp_path, replace=True)\n",
    "                            os.rename(f.name, path)\n",
    "                            all_runs_list.append(results)\n",
    "                        except:\n",
    "                            try:\n",
    "                                run = api.run(f\"{entity}/{project}/{id}\")\n",
    "                                f = run.file(\"train-output/acceptance rate_ckpt.pkl\").download(tmp_path, replace=True)\n",
    "                                os.rename(f.name, path)\n",
    "                                all_runs_list.append(results)\n",
    "                            except:\n",
    "                                print(f\"{run.name} {run.id} failed\")\n",
    "                                pass\n",
    "                else:\n",
    "                    all_runs_list.append(results)\n",
    "    df = pd.DataFrame(all_runs_list)\n",
    "    df.to_csv(f\"./results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./results.csv\")\n",
    "test_keys = []\n",
    "train_keys = []\n",
    "for c in df.columns:\n",
    "        if \"loss\" not in c and \"acc\" not in c:\n",
    "            if c.startswith(\"test\"):\n",
    "                test_keys.append(c)\n",
    "            else:\n",
    "                train_keys.append(c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                | 0/390 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STL10 SE\n",
      "Files already downloaded and verified\n",
      "500\n",
      "model wrn-16-8-stl\n",
      "Using WRN-16-8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhounie\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.21 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/chiche/advbench/notebooks/wandb/run-20220713_140052-1mv40mpg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href=\"https://wandb.ai/hounie/DAug-STL10/runs/1mv40mpg\" target=\"_blank\">SE Laplacian_DALE_PD_Reverse wrn-16-8-stl 0</a></strong> to <a href=\"https://wandb.ai/hounie/DAug-STL10\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "1it [00:01,  1.39s/it]\u001b[A\n",
      "2it [00:01,  1.16it/s]\u001b[A\n",
      "3it [00:02,  1.44it/s]\u001b[A\n",
      "4it [00:02,  1.62it/s]\u001b[A\n",
      "5it [00:03,  1.75it/s]\u001b[A\n",
      "6it [00:03,  1.84it/s]\u001b[A\n",
      "7it [00:04,  1.91it/s]\u001b[A\n",
      "8it [00:04,  1.96it/s]\u001b[A\n",
      "9it [00:05,  1.98it/s]\u001b[A\n",
      "10it [00:05,  2.01it/s]\u001b[A\n",
      "11it [00:06,  2.03it/s]\u001b[A\n",
      "12it [00:06,  2.02it/s]\u001b[A\n",
      "13it [00:07,  1.99it/s]\u001b[A\n",
      "14it [00:07,  1.96it/s]\u001b[A\n",
      "15it [00:08,  1.94it/s]\u001b[A\n",
      "16it [00:08,  1.93it/s]\u001b[A\n",
      "17it [00:09,  1.78it/s]\u001b[A\n",
      "  0%|                                                                                                                                | 0/390 [00:21<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mproject, resume\u001b[38;5;241m=\u001b[39mexp_id)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attack_name, attack \u001b[38;5;129;01min\u001b[39;00m test_attacks\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 41\u001b[0m     test_adv_acc, test_adv_acc_mean, adv_loss, accs, loss, deltas \u001b[38;5;241m=\u001b[39m \u001b[43mmisc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madv_accuracy_loss_delta\u001b[49m\u001b[43m(\u001b[49m\u001b[43malgorithm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ldr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugs_per_batch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_aug\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattack_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     train_adv_acc, train_adv_acc_mean, train_adv_loss, train_accs, train_loss, train_deltas \u001b[38;5;241m=\u001b[39m misc\u001b[38;5;241m.\u001b[39madv_accuracy_loss_delta(algorithm, val_ldr, device, attack, augs_per_batch \u001b[38;5;241m=\u001b[39m n_aug[attack_name])\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogging \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattack_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/advbench/notebooks/../advbench/lib/misc.py:183\u001b[0m, in \u001b[0;36madv_accuracy_loss_delta\u001b[0;34m(algorithm, loader, device, attack, max_batches, augs_per_batch, batched)\u001b[0m\n\u001b[1;32m    181\u001b[0m         output \u001b[38;5;241m=\u001b[39m algorithm\u001b[38;5;241m.\u001b[39mpredict(adv_imgs)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     attacked \u001b[38;5;241m=\u001b[39m \u001b[43mattack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(attacked) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    185\u001b[0m         adv_imgs, delta \u001b[38;5;241m=\u001b[39m attacked\n",
      "File \u001b[0;32m~/miniconda3/envs/daug/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/advbench/notebooks/../advbench/attacks.py:319\u001b[0m, in \u001b[0;36mRand_Aug.forward\u001b[0;34m(self, imgs, labels)\u001b[0m\n\u001b[1;32m    317\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(delta)\n\u001b[1;32m    318\u001b[0m delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mperturbation\u001b[38;5;241m.\u001b[39mclamp_delta(delta, imgs)\n\u001b[0;32m--> 319\u001b[0m adv_imgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturbation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperturb_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m adv_imgs\u001b[38;5;241m.\u001b[39mdetach(), delta\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/advbench/notebooks/../advbench/perturbations.py:15\u001b[0m, in \u001b[0;36mPerturbation.perturb_img\u001b[0;34m(self, imgs, delta)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperturb_img\u001b[39m(\u001b[38;5;28mself\u001b[39m, imgs, delta):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_perturb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/advbench/notebooks/../advbench/perturbations.py:99\u001b[0m, in \u001b[0;36mSE._perturb\u001b[0;34m(self, imgs, delta)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_perturb\u001b[39m(\u001b[38;5;28mself\u001b[39m, imgs, delta):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/advbench/notebooks/../advbench/lib/transformations.py:70\u001b[0m, in \u001b[0;36mse_transform\u001b[0;34m(imgs, delta)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mse_transform\u001b[39m(imgs, delta):\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwarp_affine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mse_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/daug/lib/python3.9/site-packages/kornia/geometry/transform/imgwarp.py:201\u001b[0m, in \u001b[0;36mwarp_affine\u001b[0;34m(src, M, dsize, mode, padding_mode, align_corners, fill_value)\u001b[0m\n\u001b[1;32m    198\u001b[0m dst_norm_trans_src_norm: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m normalize_homography(M_3x3, (H, W), dsize)\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# src_norm_trans_dst_norm = torch.inverse(dst_norm_trans_src_norm)\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m src_norm_trans_dst_norm \u001b[38;5;241m=\u001b[39m \u001b[43m_torch_inverse_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdst_norm_trans_src_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m grid \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39maffine_grid(src_norm_trans_dst_norm[:, :\u001b[38;5;241m2\u001b[39m, :], [B, C, dsize[\u001b[38;5;241m0\u001b[39m], dsize[\u001b[38;5;241m1\u001b[39m]], align_corners\u001b[38;5;241m=\u001b[39malign_corners)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m padding_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfill\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/daug/lib/python3.9/site-packages/kornia/utils/helpers.py:74\u001b[0m, in \u001b[0;36m_torch_inverse_cast\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mfloat32, torch\u001b[38;5;241m.\u001b[39mfloat64):\n\u001b[1;32m     73\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread ChkStopThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 170, in check_status\n",
      "    status_response = self._interface.communicate_stop_status()\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/site-packages/wandb/sdk/interface/interface.py\", line 127, in communicate_stop_status\n",
      "    resp = self._communicate_stop_status(status)\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 395, in _communicate_stop_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 226, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 231, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n",
      "Exception in thread NetStatThr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/threading.py\", line 973, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/threading.py\", line 910, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/site-packages/wandb/sdk/wandb_run.py\", line 152, in check_network_status\n",
      "    status_response = self._interface.communicate_network_status()\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/site-packages/wandb/sdk/interface/interface.py\", line 138, in communicate_network_status\n",
      "    resp = self._communicate_network_status(status)\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 405, in _communicate_network_status\n",
      "    resp = self._communicate(req, local=True)\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 226, in _communicate\n",
      "    return self._communicate_async(rec, local=local).get(timeout=timeout)\n",
      "  File \"/home/chiche/miniconda3/envs/daug/lib/python3.9/site-packages/wandb/sdk/interface/interface_shared.py\", line 231, in _communicate_async\n",
      "    raise Exception(\"The wandb backend process has shutdown\")\n",
      "Exception: The wandb backend process has shutdown\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "data_dir = '../advbench/data'\n",
    "device = DEVICE\n",
    "failed = []\n",
    "#algorithms = [\"MH_DALE_PD_Reverse\", \"Gaussian_DALE_PD_Reverse\", \"Laplacian_DALE_PD_Reverse\", \"Adversarial_Smoothed_MH\"]\n",
    "n_aug = {\"Rand_Aug\":100, \"Fo_Adam\":10, \"Fo_SGD\":10}\n",
    "t_attacks = [k for k in n_aug.keys()]\n",
    "for exp_id in tqdm(df[\"id\"]):\n",
    "        try:\n",
    "            api = wandb.Api(timeout=50)\n",
    "            exp = df[df[\"id\"]==exp_id]\n",
    "            dataset =  exp[\"dataset\"].values[0]\n",
    "            algorithm = \"ERM\"\n",
    "            perturbation = exp[\"perturbation\"].values[0]\n",
    "            model = exp[\"model\"].values[0]\n",
    "            if True:\n",
    "                if perturbation == \"SE\" and \"rot\" not in model:\n",
    "                    project = f\"DAug-{dataset}\"\n",
    "                    print(dataset, perturbation)\n",
    "                    t_hparams = exp[test_keys].to_dict(orient='index')[exp.index.values[0]]\n",
    "                    test_hparams = {}\n",
    "                    for k, v in t_hparams.items():\n",
    "                        test_hparams[k.replace(\"test_\",\"\")] = v\n",
    "\n",
    "                    hparams = exp[train_keys].to_dict(orient='index')[exp.index.values[0]]\n",
    "                    hparams['model'] = model\n",
    "                    hparams['epsilon'] = torch.tensor([hparams[f'epsilon_{i}'] for i in (\"rot\",\"tx\",\"ty\")]).to(device)\n",
    "                    test_hparams['epsilon'] = hparams['epsilon']\n",
    "                    hparams['batched'] = False\n",
    "                    test_hparams['batched'] = False\n",
    "                    hparams['worst_of_k_steps'] =  100\n",
    "                    test_hparams['worst_of_k_steps'] =  100\n",
    "                    aug = hparams[\"augment\"]\n",
    "                    dataset = vars(datasets)[dataset](data_dir, augmentation = aug)\n",
    "                    train_ldr, val_ldr, test_ldr = datasets.to_loaders(dataset, hparams, device=device)\n",
    "                    kw_args = {\"perturbation\": perturbation}\n",
    "                    algorithm = load_weights(exp[\"weights\"].values[0], dataset, device=DEVICE)           \n",
    "                    test_attacks = {a: vars(attacks)[a](algorithm.classifier, test_hparams, DEVICE, perturbation=\"SE\") for a in t_attacks}\n",
    "                    wandb.init(project=project, resume=exp_id)\n",
    "                    for attack_name, attack in test_attacks.items():\n",
    "                        test_adv_acc, test_adv_acc_mean, adv_loss, accs, loss, deltas = misc.adv_accuracy_loss_delta(algorithm, test_ldr, device, attack, augs_per_batch = n_aug[attack_name])\n",
    "                        train_adv_acc, train_adv_acc_mean, train_adv_loss, train_accs, train_loss, train_deltas = misc.adv_accuracy_loss_delta(algorithm, val_ldr, device, attack, augs_per_batch = n_aug[attack_name])\n",
    "                        print(f\"Logging {attack_name}...\")\n",
    "                        wandb.log({'final test_acc_adv_'+attack_name: test_adv_acc, 'final test_acc_adv_mean'+attack_name: test_adv_acc_mean, 'final test_loss_adv_'+attack_name: adv_loss,\n",
    "                        'test_loss_adv_mean_'+attack_name: loss.mean()})\n",
    "                        wandb.log({'final train_acc_adv_'+attack_name: train_adv_acc, 'final train_acc_adv_mean_'+attack_name: train_adv_acc_mean, 'final train_loss_adv_'+attack_name: train_adv_loss,\n",
    "                        'train_loss_adv_mean_'+attack_name: loss.mean()})\n",
    "                    wandb.finish(quiet=True)\n",
    "        except:\n",
    "            failed.append(exp_id)\n",
    "\n",
    "with open('failed.txt', 'w') as f:\n",
    "    for line in failed:\n",
    "        f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65f58aa27a38851e3e9850fef15fa7db5088b1b5f537a1afba292c768d907c52"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
